{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "#     print(s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (64, 64)\n",
      "ib (1, 64)\n",
      "tf.matmul(i, ix) + tf.matmul(o, im) (1, 64)\n",
      "ib (1, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    print('tf.matmul(i, ix) + tf.matmul(o, im)', (tf.matmul(i, ix) + tf.matmul(o, im)).get_shape())\n",
    "    print('ib', ib.get_shape())\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    x = tf.concat(0, outputs)\n",
    "    logits = tf.matmul(x, w) + b # tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295802 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "xtieiaswcawbth elf lbofp etmrinjwpnlt aq imxiwdeavmpl xcoan sfqqctfpgcztx xuvp y\n",
      "vl  knivelgjcx k pps  hce djnlgn nyiyyocntn eohv  hpiytfhsltknhcf fmxrlpc kpt ps\n",
      "tzmyehtivywestdm jqfscjs zoshe sasygug ulpncqoyjh n wc vruembpcxjufjivjotztlf oo\n",
      "oelodhqljwaoyo vtbneagv hhudzcddd u ehsy et laivhi hbnfwlihober b iorqt igpmwzmg\n",
      "khiqjarmotqnuzb aeer sauetrepwzsa eyyenpiek cqdpchiwka  cttqunisxqpaqwnnquoirhbb\n",
      "================================================================================\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 100: 2.597155 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.50\n",
      "Validation set perplexity: 11.49\n",
      "Average loss at step 200: 2.253040 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.24\n",
      "Validation set perplexity: 9.48\n",
      "Average loss at step 300: 2.108835 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.91\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 400: 1.999315 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500: 1.978853 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 600: 1.913205 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 700: 1.893559 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 800: 1.883478 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 900: 1.831937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 1000: 1.806884 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "================================================================================\n",
      "acre lensifor riclefter letquption in this a fuit wot fua one nine one sux remin\n",
      "ly haurs of of the eight thin roiks of useardal lepering rahaly cativior malal m\n",
      "ject of agittle windows see eugingue panic courmen systares anianiain the firigh\n",
      "s nisera ex jus tarighd anaings weob herserrogengers to desamened which lioking \n",
      "t life is is tagarer leas amplolun slemed than ispocelion chuses sulapation of c\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.797839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1200: 1.792674 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1300: 1.800633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.767386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1500: 1.771953 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1600: 1.725636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1700: 1.719262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.736748 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1900: 1.716310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2000: 1.710741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "zome compinapled cont domam camo term an internam also an ever a pepien events o\n",
      "memss with s arguring the soaded its templimise whith is emition othen the eogen\n",
      "incla eccile differmm atced an adattonsus in itont engraboouet his aga seve acon\n",
      "berglony aresed in french the was antilits and have to eutheg gom the linces the\n",
      "ong the which came and he sequen  helsend s one five two zero zero zero zero fiv\n",
      "================================================================================\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2100: 1.696837 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2200: 1.683640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2300: 1.679710 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2400: 1.691332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2500: 1.682885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2600: 1.669924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2700: 1.661377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2800: 1.661996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2900: 1.658927 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.647738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "digon uns place to re one exter such the union staltion on alagorong are all to \n",
      "legar of niwed co the or worls also not of ald transure of called in the alrorat\n",
      "badactures or sead stown water chaticiies of releared which also one deffeed abo\n",
      "ul borosfoinwashla of thomagin in there other was legology lition fioous f scope\n",
      "n colatits is startsay four were propaten beliesses proonst and amon consiciste \n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3100: 1.626700 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3200: 1.651369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3300: 1.668727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3400: 1.649223 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3500: 1.631326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3600: 1.615792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3700: 1.632292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3800: 1.655694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3900: 1.663924 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4000: 1.660932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "persons ancean during with a sist onded it out chees say trainis s finding scan \n",
      "penbegent onhers anater to history recenting was is calser early for the great t\n",
      "wind surrable use were not edion honow bs a belawing basic potulated intending t\n",
      "ble milltame enmentht of the harzy a sar interecther off is posst ane repect poi\n",
      "zer natuction by a each ake illoss aminogals is and growd be zeerows of belibble\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4100: 1.639162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4200: 1.620662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.610623 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4400: 1.641025 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.592164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4600: 1.609494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4700: 1.610966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4800: 1.600267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4900: 1.607416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5000: 1.620475 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "obinately that poturated and echilad arar usey win sea such assarund that zeired\n",
      "que wright with each time very peaphly touders that winter five nine r ghan mezb\n",
      "ndeudes autionarian frol tile masitation war gene acad ter eight kt wide to the \n",
      "er uranuman to be cogn genempumer partus mutatly la munixment conselve ispessill\n",
      "ques and geols ellace freece foreed of the setene for progations in acisment the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5100: 1.625138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.643152 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5300: 1.654103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.609097 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5500: 1.603054 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.572620 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5700: 1.580704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5800: 1.583321 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5900: 1.566276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.612012 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "s impise by polyign might thay goust major hishlone geambes of most time dna res\n",
      "mentin qua tenesis paibil rus encects muchstaubour in the and lithuate escent fr\n",
      "ted regiritish accessing informer of can thirh gredube is the mat planel chalar \n",
      " medive of in football tas they was sall senbis parted the firce bony jann ving \n",
      "ingross one nine eared during griabows lockization replacalic alcassilemary clic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100: 1.601867 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6200: 1.595896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6300: 1.569132 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6400: 1.594319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.584556 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.585554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.584142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800: 1.564619 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6900: 1.585822 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 7000: 1.586882 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ethood mosite of eriquents velement whork or in south yearual hole coccanues he \n",
      "x backands him and the sumbers traince at foin he courciens simmentionst in the \n",
      "nd two zero duside anneal dismound waltaristina snuci in is of the english of se\n",
      "nd espetation racue conside systam becablicali and datatents experentary the tha\n",
      "lests capelings as the depeciot of toprocate barlly chappley and his in the unte\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "CPU times: user 4min 37s, sys: 40.1 s, total: 5min 17s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def run():\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "        \n",
    "%time run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ax = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  am = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  ab = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "#   # Input gate: input, previous output, and bias.\n",
    "#   ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   # Forget gate: input, previous output, and bias.\n",
    "#   fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   # Memory cell: input, state and bias.                             \n",
    "#   cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   # Output gate: input, previous output, and bias.\n",
    "#   ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "#   om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "#   ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     mix = tf.matmul(i, ax)\n",
    "#     mom = tf.matmul(o, am)\n",
    "    y = tf.matmul(i, ax) + tf.matmul(o, am) + ab\n",
    "#     print('mix', mix.get_shape())\n",
    "#     print('mom', mom.get_shape())\n",
    "#     print('ab', ab.get_shape())\n",
    "#     y = mix + mom + ab\n",
    "    input_gate =  tf.sigmoid(y[:,(num_nodes * 0):(num_nodes * 1)])\n",
    "    forget_gate = tf.sigmoid(y[:,(num_nodes * 1):(num_nodes * 2)])\n",
    "    update = y[:,(num_nodes * 2):(num_nodes * 3)]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(y[:,(num_nodes * 3):(num_nodes * 4)])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "#     return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    x = tf.concat(0, outputs)\n",
    "    logits = tf.matmul(x, w) + b # tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(0, train_labels), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298068 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "igedr yddilufxcldjvzmfgngunva hm rpvozj dorpenawjwi owkmilcvumscgddcoiofmalnojm \n",
      "oev  mjrnolbx pwc  iyi kozw sbejzhsbp mckwlj jicnopfjomln hprm rotaoido  e rrszy\n",
      "tjlleqknd emn omqodvt hyfifnnyqcxys dpi  xau tl boytxgsefoboia rrknkn dmnsiosuln\n",
      "bpm  xneoiniaqom picdsmdkd twl unp toha ptgonwasb  hrbzoenbiwbpah ryqnhxzah epg \n",
      "slkerd m ilraocjdrvgauc  qse sczmaq aoopnncyiv yoapdx gp npkocjvnfkkfgystd mdnlk\n",
      "================================================================================\n",
      "Validation set perplexity: 20.42\n",
      "Average loss at step 100: 2.594666 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.49\n",
      "Validation set perplexity: 10.51\n",
      "Average loss at step 200: 2.281629 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.96\n",
      "Validation set perplexity: 9.28\n",
      "Average loss at step 300: 2.127138 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.21\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 2.015492 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 500: 1.944858 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 600: 1.888338 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 700: 1.870283 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 800: 1.826394 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 900: 1.825647 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 1000: 1.837466 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "cy expout one nine five thind glidard to estain nume dehoted to edurial expeccua\n",
      "y anamans formond ncevently origation strefs it is mayres of langs redemabon was\n",
      "life one nine nine six eight jayion uniencoud and reforan fopce zishem a a north\n",
      "est and they fings inveared the ettele beoung aconcuster are ig is was the genme\n",
      "fociss sectrams to its radia this other apliged the bitem quant unowegian fand d\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1100: 1.812365 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.782800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.782314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.757562 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.759791 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1600: 1.749098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1700: 1.740962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1800: 1.740461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1900: 1.710833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2000: 1.694523 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      " s refocl and berm to with beights nine ob dictose arions in the edves one nine \n",
      "ptian indicat at entiche one nine eight eight planly whe have sarplefone two one\n",
      "ver was which names dunal sturces ty the implecs inics some recuds dimany u anol\n",
      "y jarter were uniblels ani in a rependime havan guw and pum in holuches is conte\n",
      " perie four nine one nine nine nine eight peang and a news one eight eiz one eig\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.692402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2200: 1.685350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2300: 1.723505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.695234 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2500: 1.697490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2600: 1.695288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2700: 1.684071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.681710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2900: 1.653995 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3000: 1.650063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "tances him lausic parton ristil feal lickety must canstrussial usuties of the co\n",
      "ripa also indepaction of the sounds by relanged which was of that compharl the f\n",
      "x i ingels lounchy on hsings in the decistical sulcalin on goill studing the not\n",
      "lizity durcinewing the poenchirist part of the indisic dwitient in usele cond th\n",
      "usu cilian peops gualia reforntch triep al the prelepsuary asbas nateristion and\n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3100: 1.650458 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.643484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3300: 1.660447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3400: 1.655678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 3500: 1.648966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3600: 1.633037 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3700: 1.637951 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3800: 1.635046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3900: 1.628900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4000: 1.636137 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "pickesqeed eleat which from as taman by all munal marktem maryows it to armuerra\n",
      "terization end with in concess to beights formed they areac zero dusthy marrabre\n",
      "zar of coximated hombby unstant of a trainsion of was the after latismen irish a\n",
      "jo mannum a forceverly almost one seven nine mijult ete previct chird nceformand\n",
      "zers discenfer six during the carmor one nines a marburts dates noborpully outha\n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4100: 1.626795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4200: 1.623912 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4300: 1.626839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4400: 1.589918 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4500: 1.607989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4600: 1.626440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4700: 1.606538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.611509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4900: 1.599991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 5000: 1.609541 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "unked kyjy are irenity by the mouns record ten compoter rampogosit peared from f\n",
      "zus new r spartium heading are arever of aselem in the gams unstant deach films \n",
      "quenawell as actman as thoilging the specis in the was the tranting ever in the \n",
      " see music he ise to the presensy buterve semoral in urean hanilan to the demove\n",
      "quentifica cractory font after action is of noxtherent hessaign as urpes of a di\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5100: 1.598080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5200: 1.572903 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5300: 1.563914 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5400: 1.605564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5500: 1.568258 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5600: 1.586382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5700: 1.569614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5800: 1.601781 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5900: 1.600580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6000: 1.560124 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "================================================================================\n",
      "she largest one two six eight oresquides is members howivitite conmined from the\n",
      "valcs his and with they s catraside contempured that assuce unlertion can be ver\n",
      "y enthereves of full to byite stowas partiliam one nine eight six jubel common a\n",
      "smested of from officationiol contair sould polite arens one nine willd manalk l\n",
      "x leyther beekation include as also one nine nine two two celtads deferration ov\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6100: 1.555981 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6200: 1.574649 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6300: 1.609383 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6400: 1.576846 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6500: 1.580297 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6600: 1.607382 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6700: 1.604733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6800: 1.540613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6900: 1.556723 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 7000: 1.585501 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "lationaly americanic relatic even protob had anabilis of gii ene wet hetsian sur\n",
      "tics dates empranchully logon gostaes at new four one nine eight in the two zero\n",
      "ch wishwas homest internations conveltable of the six with presix after these gr\n",
      "pre a commooc is nor rany owownd to for the moallist foom its degition to hamand\n",
      "rimated the numersmanfel the wargh seass clain supperect the four iscre shayarch\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "CPU times: user 4min 37s, sys: 40.5 s, total: 5min 18s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def run():\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "        \n",
    "%time run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
